# YouTube RAG Chrome Extension Backend

This is the backend service for the YouTube RAG Chrome Extension. It provides the capabilities to ingest YouTube video transcripts, store their embeddings, and perform Retrieval-Augmented Generation (RAG) to answer questions based entirely on the video's content.

## RAG Architecture

The backend handles the entire document pipeline from transcript ingestion to context-aware answer generation. It is built primarily using **FastAPI** for API routing and **LangChain** for orchestrating the LLM, vector store, and document components.

### 1. Ingestion & Preprocessing
* **Transcript & Metadata Fetching**: We use `youtube-transcript-api` to capture the timestamped transcript of a video. We use the YouTube Data API v3 (`googleapiclient.discovery`) to grab vital metadata (title, channel, views).
* **Chunking**: LangChain's `RecursiveCharacterTextSplitter` is utilized. We chunk the transcript at **1500 characters** with an **overlap of 300 characters**. This size provides enough linguistic context within each chunk while avoiding exceeding embedding token limits.

### 2. Embeddings
* **Hugging Face Text Embeddings**: We use the `BAAI/bge-small-en-v1.5` model via `HuggingFaceEmbeddings` locally on the CPU.
* **Why Hugging Face BGE?**: Using an open-source model like BGE Small guarantees excellent semantic retrieval performance parity with heavy commercial models, while keeping API embedding costs at exactly zero and execution blazing fast.

### 3. Vector Database
* **Qdrant**: All document chunks and metadata are stored in a Qdrant collection named `youtube_videos`. 
* **Filtering**: Payload indexes are created based on the `video_id`. This allows rapid filtering down to just the single specific YouTube video the user is asking about. 

### 4. Retrieval & Query Rewriting
* **Query Rewriting**: Before searching the vector database, the user query is parsed by the LLM and rewritten into a pure, keyword-dense search query. This eliminates conversational filler ("Can you tell me if...") that degrades vector similarity search.
* **Similarity Search**: The rewritten query fetches the top **25 closest document chunks** from Qdrant, using Cosine Distance.

### 5. Generation
* **LLM Model**: We utilize Google's **Gemini 2.5 Flash** (`gemini-2.5-flash`).
* **Why Gemini?**: Gemini 2.5 Flash provides exceptional context-window capabilities and blazing generation speeds. It accurately handles the substantial context strings generated by 25 document chunks and returns fluid, heavily-cited natural language.
* **Chronological Sorting**: Before inserting chunks into the prompt, the top 25 chunks are sorted chronologically by their timestamps. This preserves the narrative flow of the video for the LLM.

## Setup Instructions

### Prerequisites
* Python 3.8+
* A running **Qdrant** instance (using Docker or Qdrant Cloud)
* A Google API Key (for Gemini)
* A YouTube Data API v3 Key

### 1. Clone & Environment setup
```bash
# Navigate to the backend directory
cd youtube_rag

# Create a virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Environment Variables
Create a `.env` file in the root of the `youtube_rag` directory and configure the following variables:
```env
# Required APIs
YOUTUBE_API_KEY="your_youtube_data_api_key_here"
GEMINI_API_KEY="your_google_gemini_api_key_here"

# Qdrant Configuration
QDRANT_URL="http://localhost:6333" # Or your Qdrant Cloud URL
QDRANT_API_KEY="your_qdrant_cloud_key_if_applicable"

# Optional Server Settings
PORT=8000
LLM_MODEL="gemini-2.5-flash"
```

### 4. Running the Server
You can start the backend by running the main Python file directly:
```bash
python main.py
```
Or via Uvicorn:
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```
The API should now be running locally on `http://localhost:8000`.
